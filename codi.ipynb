{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practica APA - predicció superfície cremada d'un incendi\n",
    "Sergi Curto Panisello,\n",
    "Joan Melchor Lladó\n",
    "\n",
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "everything = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtenim les dades\n",
    "Procedim a agafar les dades en format csv descarregades de https://datos.civio.es/dataset/todos-los-incendios-forestales/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         superficie           lat           lng  latlng_explicit  \\\n",
      "count  82640.000000  82616.000000  82616.000000     82640.000000   \n",
      "mean      19.888085     41.763721     -5.664360         0.741626   \n",
      "std      223.787536      2.019672      4.394031         0.437743   \n",
      "min        1.000000      0.490720  -1000.000000         0.000000   \n",
      "25%        1.500000     40.876544     -7.257298         0.000000   \n",
      "50%        3.000000     42.371123     -6.019478         1.000000   \n",
      "75%        7.150000     43.143145     -4.303078         1.000000   \n",
      "max    28879.100000     87.824157    242.755603         1.000000   \n",
      "\n",
      "        idcomunidad   idprovincia   idmunicipio         causa  causa_supuesta  \\\n",
      "count  82640.000000  82640.000000  82640.000000  82640.000000         46465.0   \n",
      "mean       7.850278     28.411375     77.795946      3.695051             1.0   \n",
      "std        5.419922     11.727155     98.867769      1.032923             0.0   \n",
      "min        1.000000      1.000000      1.000000      1.000000             1.0   \n",
      "25%        3.000000     21.000000     27.000000      4.000000             1.0   \n",
      "50%        5.000000     32.000000     52.000000      4.000000             1.0   \n",
      "75%       14.000000     36.000000     92.000000      4.000000             1.0   \n",
      "max       18.000000     51.000000    999.000000      6.000000             1.0   \n",
      "\n",
      "         causa_desc      muertos      heridos      time_ctrl       time_ext  \\\n",
      "count  82640.000000  2724.000000  3071.000000   82640.000000   82640.000000   \n",
      "mean      12.216917     0.021292     0.197981     236.247302     523.010733   \n",
      "std       24.631048     0.298586     0.716858     901.186810    2560.985781   \n",
      "min        0.000000     0.000000     0.000000       0.000000       0.000000   \n",
      "25%        0.000000     0.000000     0.000000      64.000000     134.000000   \n",
      "50%        2.000000     0.000000     0.000000     118.000000     221.000000   \n",
      "75%       10.000000     0.000000     0.000000     210.000000     415.000000   \n",
      "max       99.000000    11.000000    12.000000  132555.000000  529682.000000   \n",
      "\n",
      "           personal        medios        gastos      perdidas  \n",
      "count  82640.000000  82640.000000  1.162400e+04  3.434900e+04  \n",
      "mean      21.188093      3.143224  7.500955e+03  3.230666e+04  \n",
      "std       48.210975      6.180303  3.939254e+04  4.315912e+05  \n",
      "min        0.000000      0.000000  0.000000e+00 -2.896100e+04  \n",
      "25%        5.000000      1.000000  3.560000e+02  1.020000e+02  \n",
      "50%       11.000000      2.000000  1.107000e+03  1.323000e+03  \n",
      "75%       23.000000      3.000000  3.682750e+03  5.738000e+03  \n",
      "max     3979.000000    310.000000  1.426641e+06  3.064011e+07  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fires-all.csv', index_col='id')\n",
    "print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tractament de missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lat', 'lng', 'causa_supuesta', 'muertos', 'heridos', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "# Columnes amb missing values, també es pot veure al describe del\n",
    "# dataset a les variables que no tinguin un count de 82640\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar lat i long\n",
    "\n",
    "Per arreglar lat i long ho fem en un altre document \"corregirCoordenades.py\", ja que és un procés més lent.\n",
    "Es dedica a comprovar les coordenades de cada instància per veure si es corresponen a un diccionari generat\n",
    "anteriorment amb el fitxer \"obtenirCoordenades.py\". Hi ha una explicació més extensa a la documentació."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['causa_supuesta', 'muertos', 'heridos', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('coordsCorregides.csv', index_col='id')\n",
    "# Comprovem que ja no queden nulls a coordenades\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar muertos, heridos y causa_supuesta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Comencem per emplenar els missing values de muertos i heridos ja que\n",
    "# si no hi han dades suposarem que són 0.\n",
    "df['muertos'].fillna(0, inplace=True)\n",
    "df['heridos'].fillna(0, inplace=True)\n",
    "# Sobre la \"causa supuesta\" és normal que hi hagin instàncies sense valor ja que és o 1 o nan per tant els posem a 0\n",
    "df['causa_supuesta'].fillna(0, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comprovar provincies i comunitats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Municipio a la mateixa provincia Empty DataFrame\n",
      "Columns: [municipio]\n",
      "Index: []\n",
      "Provincia a la mateixa comunitat Empty DataFrame\n",
      "Columns: [idprovincia]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandasql as ps\n",
    "\n",
    "q1 = \"SELECT municipio  FROM df GROUP BY municipio HAVING COUNT(DISTINCT idprovincia) > 1;\"\n",
    "q2 = \"SELECT idprovincia FROM df GROUP BY idprovincia HAVING COUNT(DISTINCT idcomunidad) > 1;\"\n",
    "\n",
    "print(\"Municipio a la mateixa provincia\", ps.sqldf(q1))\n",
    "print(\"Provincia a la mateixa comunitat\", ps.sqldf(q2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar gastos y perdidas\n",
    "\n",
    "Per a fer aquesta part hem de netejar una mica més el dataset, la columna de \"idmunicipio\" és incorrecte, ja que\n",
    "diferents municipis comparteixen el mateix id tot i està a cada punta del territori. També eliminem la columna municipio\n",
    "ja que amb la latitud i longitud ja sabem on està localitzat l'incendi.\n",
    "\n",
    "Primer separem target del dataset, volem predir la superfície cremada donat un incendi per\n",
    "tant separem aquesta variable de la resta.\n",
    "\n",
    "https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Arreglar valors massa petits posant-lo a NaN\n",
    "df.at[df.loc[((df[\"gastos\"] < 25) & (df[\"gastos\"] != 0))].index, 'gastos'] = np.NaN\n",
    "df.at[df.loc[((df[\"perdidas\"] < 25) & (df[\"perdidas\"] != 0))].index, 'perdidas'] = np.NaN\n",
    "# Valors massa extranys\n",
    "df.at[df.loc[(df[\"perdidas\"] == 999999)].index, 'perdidas'] = np.NaN\n",
    "df.at[df.loc[(df[\"gastos\"] == 999999)].index, 'gastos'] = np.NaN\n",
    "\n",
    "# Convertir fecha en un numero en comptes de string\n",
    "df[\"fecha\"] = df[\"fecha\"].apply(lambda f: f.replace('-', ''))\n",
    "# Esborrem columnes innecessàries\n",
    "df.drop(['idmunicipio', 'municipio', 'idcomunidad'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Corregir valors de time ext i time control ja que no podem tenir que s'ha tardat més en controlar\n",
    "# que en extendre. També treiem els que tenen valor de 0 ja que sino, no podrem aplicar la següent formula\n",
    "df.at[df.loc[(df[\"time_ctrl\"] > df[\"time_ext\"])].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "df.at[df.loc[(df[\"time_ctrl\"] == 0) | (df[\"time_ext\"] == 0)].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "# Comprovem que el temps de control i d'extensió és adequat per les hectàrees de l'incendi\n",
    "df.at[df.loc[(df[\"time_ctrl\"] / df[\"superficie\"] < 20)].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "\n",
    "df = pd.get_dummies(data=df, columns=[\"causa\", \"causa_desc\", \"idprovincia\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time_ctrl', 'time_ext', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerMinMaxKNNImputer = MinMaxScaler()\n",
    "df = pd.DataFrame(scalerMinMaxKNNImputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "if everything:\n",
    "    # # # Apliquem KNN per tal d'emplenar els missing values\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=1, copy=False)\n",
    "    pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "    df.describe()\n",
    "    print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "    df.to_csv(\"noNaNDataFrame.csv\")\n",
    "    print(df.head())\n",
    "    print(df.describe())\n",
    "\n",
    "df = pd.read_csv('noNaNDataFrame.csv', index_col=0)\n",
    "df = pd.DataFrame(scalerMinMaxKNNImputer.inverse_transform(df), columns=df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         4.000000\n",
      "1         0.872727\n",
      "2         1.777778\n",
      "3         1.074627\n",
      "4        14.666667\n",
      "           ...    \n",
      "82522    24.761905\n",
      "82523     9.363057\n",
      "82524    13.043478\n",
      "82525    22.352941\n",
      "82526    11.473684\n",
      "Name: PeoplePerHour, Length: 82527, dtype: float64\n",
      "0        0.800000\n",
      "1        0.218182\n",
      "2        0.444444\n",
      "3        0.179104\n",
      "4        2.666667\n",
      "           ...   \n",
      "82522    2.857143\n",
      "82523    0.955414\n",
      "82524    1.739130\n",
      "82525    4.705882\n",
      "82526    1.017544\n",
      "Name: MediosPerHour, Length: 82527, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['PeoplePerHour'] = df.personal/(df.time_ctrl/60)\n",
    "df['MediosPerHour'] = df.medios/(df.time_ctrl/60)\n",
    "print(df['PeoplePerHour'])\n",
    "print(df['MediosPerHour'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scaling and normalization of Dataset\n",
    "\n",
    "També fem el train, test, validation split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X = df.drop(\"superficie\", axis=1)\n",
    "y = df[\"superficie\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "# StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdScaler = StandardScaler()\n",
    "stdScaler.fit(X_train)\n",
    "aux_train = stdScaler.transform(X_train.values)\n",
    "X_train_std = pd.DataFrame(aux_train, index=X_train.index, columns=X_train.columns)\n",
    "aux_test = stdScaler.transform(X_test.values)\n",
    "X_test_std = pd.DataFrame(aux_test, index=X_test.index, columns=X_test.columns)\n",
    "aux_val = stdScaler.transform(X_val.values)\n",
    "X_val_std = pd.DataFrame(aux_val, index=X_val.index, columns=X_val.columns)\n",
    "\n",
    "# MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerMinMaxKNNImputer = MinMaxScaler()\n",
    "scalerMinMaxKNNImputer.fit(X_train)\n",
    "aux_train = scalerMinMaxKNNImputer.transform(X_train.values)\n",
    "X_train_minMax = pd.DataFrame(aux_train, index=X_train.index, columns=X_train.columns)\n",
    "aux_test = scalerMinMaxKNNImputer.transform(X_test.values)\n",
    "X_test_minMax = pd.DataFrame(aux_test, index=X_test.index, columns=X_test.columns)\n",
    "aux_val = scalerMinMaxKNNImputer.transform(X_val.values)\n",
    "X_val_minMax = pd.DataFrame(aux_val, index=X_val.index, columns=X_val.columns)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train)\n",
    "aux_train = scalerMinMaxKNNImputer.transform(X_train.values)\n",
    "X_train_norm = pd.DataFrame(aux_train, index=X_train.index, columns=X_train.columns)\n",
    "aux_test = scalerMinMaxKNNImputer.transform(X_test.values)\n",
    "X_test_norm = pd.DataFrame(aux_test, index=X_test.index, columns=X_test.columns)\n",
    "aux_val = scalerMinMaxKNNImputer.transform(X_val.values)\n",
    "X_val_norm = pd.DataFrame(aux_val, index=X_val.index, columns=X_val.columns)\n",
    "\n",
    "# Without some of one hot encoding variables\n",
    "noOneHotXTrain = X_train.drop(X_train.filter(regex='^idprovincia').columns, axis=1)\n",
    "noOneHotXVal = X_val.drop(X_val.filter(regex='^idprovincia').columns, axis=1)\n",
    "noOneHotXTest = X_test.drop(X_test.filter(regex='^idprovincia').columns, axis=1)\n",
    "noOneHotXTrain = noOneHotXTrain.drop(noOneHotXTrain.filter(regex='^causa').columns, axis=1)\n",
    "noOneHotXVal = noOneHotXVal.drop(noOneHotXVal.filter(regex='^causa').columns, axis=1)\n",
    "noOneHotXTest = noOneHotXTest.drop(noOneHotXTest.filter(regex='^causa').columns, axis=1)\n",
    "\n",
    "# MinMaxScaler\n",
    "scalerMinMaxKNNImputer = MinMaxScaler()\n",
    "scalerMinMaxKNNImputer.fit(noOneHotXTrain)\n",
    "aux_train = scalerMinMaxKNNImputer.transform(noOneHotXTrain.values)\n",
    "X_train_minMax_noHot = pd.DataFrame(aux_train, index=noOneHotXTrain.index, columns=noOneHotXTrain.columns)\n",
    "aux_test = scalerMinMaxKNNImputer.transform(noOneHotXTest.values)\n",
    "X_test_minMax_noHot = pd.DataFrame(aux_test, index=noOneHotXTest.index, columns=noOneHotXTest.columns)\n",
    "aux_val = scalerMinMaxKNNImputer.transform(noOneHotXVal.values)\n",
    "X_val_minMax_noHot = pd.DataFrame(aux_val, index=noOneHotXVal.index, columns=noOneHotXVal.columns)\n",
    "\n",
    "\n",
    "# print(X_train.head(10))\n",
    "# print(X_train_std.head(10))\n",
    "# print(X_train_minMax.head(10))\n",
    "\n",
    "trainDataframes = [X_train, X_train_std, X_train_minMax, X_train_norm, noOneHotXTrain, X_train_minMax_noHot]\n",
    "testDataframes = [X_test, X_test_std, X_test_minMax, X_test_norm, noOneHotXTest, X_test_minMax_noHot]\n",
    "valDataframes = [X_val, X_val_std, X_val_minMax, X_val_norm, noOneHotXVal, X_val_minMax_noHot]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  R squared            time\n",
      "Linear Regression  0.140283  0:00:00.950382\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(index=['Linear Regression'], columns=['R squared', 'time'])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=False, n_jobs=-1)\n",
    "\n",
    "# [X_train, X_train_std, X_train_minMax, X_train_norm, noOneHotXTrain, X_train_minMax_noHot]\n",
    "dataLr = 0\n",
    "start = time()\n",
    "lr.fit(trainDataframes[dataLr], y_train)\n",
    "training_time = time() - start\n",
    "\n",
    "# y_val_predicted = lr.predict(X_test)\n",
    "# r2_score_lr = lr.score(X_val_norm, y_val)\n",
    "#\n",
    "# print('Linear regression' ,r2_score_lr)\n",
    "\n",
    "scores = cross_val_score(lr, valDataframes[dataLr], y_val, cv=5)\n",
    "acc=np.mean(scores)\n",
    "results_df.loc['Linear Regression',:] = [acc, timedelta(seconds=training_time)]\n",
    "print(results_df)\n",
    "\n",
    "# Original =                0.140283\n",
    "# Std =                     -5.25768e+22\n",
    "# MinMax =                  -4.04218e+19\n",
    "# Norm =                    -4.04218e+19\n",
    "# noOneHotXTrain =          0.12905\n",
    "# X_train_minMax_noHot =    0.12905"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  R squared            time\n",
      "Linear Regression  0.140283  0:00:00.950382\n",
      "KNN                0.229668  0:00:00.450912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# [X_train, X_train_std, X_train_minMax, X_train_norm, noOneHotXTrain, X_train_minMax_noHot]\n",
    "dataKNN = 5\n",
    "\n",
    "X_train_quick = trainDataframes[dataKNN].head(5000)\n",
    "y_train_quick = y_train.head(5000)\n",
    "\n",
    "res = []\n",
    "\n",
    "if everything:\n",
    "\n",
    "    knnR = KNeighborsRegressor()\n",
    "    # distance euclidean 9\n",
    "    clf = GridSearchCV(estimator=knnR,\n",
    "                             param_grid = {'weights':('uniform', 'distance'),\n",
    "                                                       'n_neighbors':list(range(1,30)),\n",
    "                                                       'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'\n",
    "                                                , 'wminkowski', 'seuclidean', 'mahalanobis']},\n",
    "                             n_jobs=-1,\n",
    "                             return_train_score=True)\n",
    "    # clf = RandomizedSearchCV(estimator=knnR,\n",
    "    #                          param_distributions= dict(weights = ('uniform', 'distance'),\n",
    "    #                                                    n_neighbors = list(range(1, 40)),\n",
    "    #                                                    metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski'\n",
    "    #                                             , 'wminkowski', 'seuclidean', 'mahalanobis']),\n",
    "    #                          n_jobs=-1,\n",
    "    #                          return_train_score=True,\n",
    "    #                          n_iter=10)\n",
    "    modelCV = clf.fit(X_train_quick, y_train_quick)\n",
    "    modelCV.best_params_ , modelCV.best_score_\n",
    "    y_pred = modelCV.predict(testDataframes[dataKNN].head(100))\n",
    "    print('Resultat test', modelCV.score(testDataframes[dataKNN], y_test))\n",
    "    print('Resultat val:', modelCV.score(valDataframes[dataKNN], y_val))\n",
    "\n",
    "    # Resultat test 0.14673539627333576\n",
    "    # Resultat val: 0.2159892244417193\n",
    "    # ({'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'distance'}, 0.2750767350186984)\n",
    "\n",
    "knn = KNeighborsRegressor(metric = 'euclidean', n_neighbors = 9, weights = 'distance',n_jobs=-1)\n",
    "init_time = time()\n",
    "knn.fit(trainDataframes[dataKNN], y_train)\n",
    "training_time = time()-init_time\n",
    "\n",
    "scores = cross_val_score(knn, valDataframes[dataKNN], y_val, cv=5)\n",
    "acc=np.mean(scores)\n",
    "results_df.loc['KNN',:] = [acc, timedelta(seconds=training_time)]\n",
    "print(results_df)\n",
    "\n",
    "# Original =                -0.364093\n",
    "# Std =                     0.201094\n",
    "# MinMax =                  -0.0994702\n",
    "# Norm =                    -0.0994702\n",
    "# noOneHotXTrain =          -0.364093\n",
    "# X_train_minMax_noHot =    0.229668"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Regressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-63-4ef4734aef55>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m                          n_iter=1)\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mmodelCV\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainDataframes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdataKNN\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     70\u001B[0m                           FutureWarning)\n\u001B[1;32m     71\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0marg\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    734\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 736\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    737\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    738\u001B[0m         \u001B[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36m_run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1527\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1528\u001B[0m         \u001B[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1529\u001B[0;31m         evaluate_candidates(ParameterSampler(\n\u001B[0m\u001B[1;32m   1530\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparam_distributions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1531\u001B[0m             random_state=self.random_state))\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mevaluate_candidates\u001B[0;34m(candidate_params)\u001B[0m\n\u001B[1;32m    706\u001B[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001B[1;32m    707\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 708\u001B[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001B[0m\u001B[1;32m    709\u001B[0m                                                        \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    710\u001B[0m                                                        \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1059\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1060\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1061\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1062\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1063\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    938\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    939\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 940\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    941\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    942\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[1;32m    541\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 542\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    543\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    432\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 434\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    435\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    436\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mCANCELLED\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCANCELLED_AND_NOTIFIED\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 302\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    303\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# [X_train, X_train_std, X_train_minMax, X_train_norm, noOneHotXTrain, X_train_minMax_noHot]\n",
    "dataRfR = 0\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "# clf = RandomizedSearchCV(estimator=regr,\n",
    "#                          param_distributions= dict(criterion = ('mse', 'mae'),\n",
    "#                                                    max_depth = list(range(2, 40)),\n",
    "#                                                    n_estimators = list(range(30, 50))),\n",
    "#                          n_jobs=-1,\n",
    "#                          n_iter=1)\n",
    "#\n",
    "# modelCV = clf.fit(trainDataframes[dataKNN], y_train)\n",
    "modelCV = regr.fit(trainDataframes[dataRfR])\n",
    "\n",
    "from sklearn import metrics\n",
    "# print('Resultat ', modelCV.score(valDataframes[dataRfR], y_val))\n",
    "\n",
    "modelCV.best_params_ , modelCV.best_score_\n",
    "\n",
    "scores = cross_val_score(modelCV, valDataframes[dataRfR], y_val, cv=5)\n",
    "acc=np.mean(scores)\n",
    "results_df.loc['RandomForestRegressor',:] = [acc, timedelta(seconds=training_time)]\n",
    "print(results_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "print(regr.predict(X_test[:2]))\n",
    "print(regr.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(regr.score(X_val, y_val))\n",
    "print(regr.score(X_train, y_train))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}