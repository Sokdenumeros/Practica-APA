{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practica APA - predicció superfície cremada d'un incendi\n",
    "Sergi Curto Panisello,\n",
    "Joan Melchor Lladó\n",
    "\n",
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "everything = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtenim les dades\n",
    "Procedim a agafar les dades en format csv descarregades de https://datos.civio.es/dataset/todos-los-incendios-forestales/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         superficie           lat           lng  latlng_explicit  \\\n",
      "count  82640.000000  82616.000000  82616.000000     82640.000000   \n",
      "mean      19.888085     41.763721     -5.664360         0.741626   \n",
      "std      223.787536      2.019672      4.394031         0.437743   \n",
      "min        1.000000      0.490720  -1000.000000         0.000000   \n",
      "25%        1.500000     40.876544     -7.257298         0.000000   \n",
      "50%        3.000000     42.371123     -6.019478         1.000000   \n",
      "75%        7.150000     43.143145     -4.303078         1.000000   \n",
      "max    28879.100000     87.824157    242.755603         1.000000   \n",
      "\n",
      "        idcomunidad   idprovincia   idmunicipio         causa  causa_supuesta  \\\n",
      "count  82640.000000  82640.000000  82640.000000  82640.000000         46465.0   \n",
      "mean       7.850278     28.411375     77.795946      3.695051             1.0   \n",
      "std        5.419922     11.727155     98.867769      1.032923             0.0   \n",
      "min        1.000000      1.000000      1.000000      1.000000             1.0   \n",
      "25%        3.000000     21.000000     27.000000      4.000000             1.0   \n",
      "50%        5.000000     32.000000     52.000000      4.000000             1.0   \n",
      "75%       14.000000     36.000000     92.000000      4.000000             1.0   \n",
      "max       18.000000     51.000000    999.000000      6.000000             1.0   \n",
      "\n",
      "         causa_desc      muertos      heridos      time_ctrl       time_ext  \\\n",
      "count  82640.000000  2724.000000  3071.000000   82640.000000   82640.000000   \n",
      "mean      12.216917     0.021292     0.197981     236.247302     523.010733   \n",
      "std       24.631048     0.298586     0.716858     901.186810    2560.985781   \n",
      "min        0.000000     0.000000     0.000000       0.000000       0.000000   \n",
      "25%        0.000000     0.000000     0.000000      64.000000     134.000000   \n",
      "50%        2.000000     0.000000     0.000000     118.000000     221.000000   \n",
      "75%       10.000000     0.000000     0.000000     210.000000     415.000000   \n",
      "max       99.000000    11.000000    12.000000  132555.000000  529682.000000   \n",
      "\n",
      "           personal        medios        gastos      perdidas  \n",
      "count  82640.000000  82640.000000  1.162400e+04  3.434900e+04  \n",
      "mean      21.188093      3.143224  7.500955e+03  3.230666e+04  \n",
      "std       48.210975      6.180303  3.939254e+04  4.315912e+05  \n",
      "min        0.000000      0.000000  0.000000e+00 -2.896100e+04  \n",
      "25%        5.000000      1.000000  3.560000e+02  1.020000e+02  \n",
      "50%       11.000000      2.000000  1.107000e+03  1.323000e+03  \n",
      "75%       23.000000      3.000000  3.682750e+03  5.738000e+03  \n",
      "max     3979.000000    310.000000  1.426641e+06  3.064011e+07  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fires-all.csv', index_col='id')\n",
    "print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tractament de missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lat', 'lng', 'causa_supuesta', 'muertos', 'heridos', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "# Columnes amb missing values, també es pot veure al describe del\n",
    "# dataset a les variables que no tinguin un count de 82640\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar lat i long\n",
    "\n",
    "Per arreglar lat i long ho fem en un altre document \"corregirCoordenades.py\", ja que és un procés més lent.\n",
    "Es dedica a comprovar les coordenades de cada instància per veure si es corresponen a un diccionari generat\n",
    "anteriorment amb el fitxer \"obtenirCoordenades.py\". Hi ha una explicació més extensa a la documentació."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['causa_supuesta', 'muertos', 'heridos', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('coordsCorregides.csv', index_col='id')\n",
    "# Comprovem que ja no queden nulls a coordenades\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar muertos, heridos y causa_supuesta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# Comencem per emplenar els missing values de muertos i heridos ja que\n",
    "# si no hi han dades suposarem que són 0.\n",
    "df['muertos'].fillna(0, inplace=True)\n",
    "df['heridos'].fillna(0, inplace=True)\n",
    "# Sobre la \"causa supuesta\" és normal que hi hagin instàncies sense valor ja que és o 1 o nan per tant els posem a 0\n",
    "df['causa_supuesta'].fillna(0, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comprovar provincies i comunitats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Municipio a la mateixa provincia Empty DataFrame\n",
      "Columns: [municipio]\n",
      "Index: []\n",
      "Provincia a la mateixa comunitat Empty DataFrame\n",
      "Columns: [idprovincia]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandasql as ps\n",
    "\n",
    "q1 = \"SELECT municipio  FROM df GROUP BY municipio HAVING COUNT(DISTINCT idprovincia) > 1;\"\n",
    "q2 = \"SELECT idprovincia FROM df GROUP BY idprovincia HAVING COUNT(DISTINCT idcomunidad) > 1;\"\n",
    "\n",
    "print(\"Municipio a la mateixa provincia\", ps.sqldf(q1))\n",
    "print(\"Provincia a la mateixa comunitat\", ps.sqldf(q2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arreglar gastos y perdidas\n",
    "\n",
    "Per a fer aquesta part hem de netejar una mica més el dataset, la columna de \"idmunicipio\" és incorrecte, ja que\n",
    "diferents municipis comparteixen el mateix id tot i està a cada punta del territori. També eliminem la columna municipio\n",
    "ja que amb la latitud i longitud ja sabem on està localitzat l'incendi.\n",
    "\n",
    "Primer separem target del dataset, volem predir la superfície cremada donat un incendi per\n",
    "tant separem aquesta variable de la resta.\n",
    "\n",
    "https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# Arreglar valors massa petits posant-lo a NaN\n",
    "df.at[df.loc[((df[\"gastos\"] < 25) & (df[\"gastos\"] != 0))].index, 'gastos'] = np.NaN\n",
    "df.at[df.loc[((df[\"perdidas\"] < 25) & (df[\"perdidas\"] != 0))].index, 'perdidas'] = np.NaN\n",
    "# Valors massa extranys\n",
    "df.at[df.loc[(df[\"perdidas\"] == 999999)].index, 'perdidas'] = np.NaN\n",
    "df.at[df.loc[(df[\"gastos\"] == 999999)].index, 'gastos'] = np.NaN\n",
    "\n",
    "# Convertir fecha en un numero en comptes de string\n",
    "df[\"fecha\"] = df[\"fecha\"].apply(lambda f: f.replace('-', ''))\n",
    "# Esborrem columnes innecessàries\n",
    "df.drop(['idmunicipio', 'municipio', 'idcomunidad'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# Corregir valors de time ext i time control ja que no podem tenir que s'ha tardat més en controlar\n",
    "# que en extendre. També treiem els que tenen valor de 0 ja que sino, no podrem aplicar la següent formula\n",
    "df.at[df.loc[(df[\"time_ctrl\"] > df[\"time_ext\"])].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "df.at[df.loc[(df[\"time_ctrl\"] == 0) | (df[\"time_ext\"] == 0)].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "# Comprovem que el temps de control i d'extensió és adequat per les hectàrees de l'incendi\n",
    "df.at[df.loc[(df[\"time_ctrl\"] / df[\"superficie\"] < 20)].index, ['time_ctrl', 'time_ext']] = np.NaN\n",
    "\n",
    "df = pd.get_dummies(data=df, columns=[\"causa\", \"causa_desc\", \"idprovincia\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time_ctrl', 'time_ext', 'gastos', 'perdidas']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "if everything:\n",
    "    # # # Apliquem KNN per tal d'emplenar els missing values\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=1, copy=False)\n",
    "    pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "    df.describe()\n",
    "    print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "    df.to_csv(\"noNaNDataFrame.csv\")\n",
    "    print(df.head())\n",
    "    print(df.describe())\n",
    "\n",
    "df = pd.read_csv('noNaNDataFrame.csv', index_col=0)\n",
    "df = pd.DataFrame(scaler.inverse_transform(df), columns=df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         4.000000\n",
      "1         0.872727\n",
      "2         1.777778\n",
      "3         1.074627\n",
      "4        14.666667\n",
      "           ...    \n",
      "82522    24.761905\n",
      "82523     9.363057\n",
      "82524    13.043478\n",
      "82525    22.352941\n",
      "82526    11.473684\n",
      "Name: PeoplePerHour, Length: 82527, dtype: float64\n",
      "0        0.800000\n",
      "1        0.218182\n",
      "2        0.444444\n",
      "3        0.179104\n",
      "4        2.666667\n",
      "           ...   \n",
      "82522    2.857143\n",
      "82523    0.955414\n",
      "82524    1.739130\n",
      "82525    4.705882\n",
      "82526    1.017544\n",
      "Name: MediosPerHour, Length: 82527, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['PeoplePerHour'] = df.personal/(df.time_ctrl/60)\n",
    "df['MediosPerHour'] = df.medios/(df.time_ctrl/60)\n",
    "print(df['PeoplePerHour'])\n",
    "print(df['MediosPerHour'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression 0.32552703078245804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(\"superficie\", axis=1)\n",
    "y = df[\"superficie\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_features = scaler.transform(X_train.values)\n",
    "X_train_stdScal = pd.DataFrame(scaled_features, index=X_train.index, columns=X_train.columns)\n",
    "scaled_features = scaler.transform(X_test.values)\n",
    "X_test_stdScal = pd.DataFrame(scaled_features, index=X_test.index, columns=X_test.columns)\n",
    "scaled_features = scaler.transform(X_val.values)\n",
    "X_val_stdScal = pd.DataFrame(scaled_features, index=X_val.index, columns=X_val.columns)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=False, n_jobs=-1)\n",
    "\n",
    "lr.fit(X_train_stdScal, y_train)\n",
    "\n",
    "y_val_predicted = lr.predict(X_val_stdScal)\n",
    "r2_score_lr = lr.score(X_val_stdScal, y_val)\n",
    "\n",
    "print('Linear regression' ,r2_score_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression without one hot encoding 0.3242303459254017\n"
     ]
    }
   ],
   "source": [
    "noOneHotXTrain = X_train.drop(X_train.filter(regex='^idprovincia').columns, axis=1)\n",
    "noOneHotXVal = X_val.drop(X_val.filter(regex='^idprovincia').columns, axis=1)\n",
    "noOneHotXTrain = noOneHotXTrain.drop(noOneHotXTrain.filter(regex='^causa_desc').columns, axis=1)\n",
    "noOneHotXVal = noOneHotXVal.drop(noOneHotXVal.filter(regex='^causa_desc').columns, axis=1)\n",
    "\n",
    "lr = LinearRegression(normalize=False, n_jobs=-1)\n",
    "\n",
    "lr.fit(noOneHotXTrain, y_train)\n",
    "\n",
    "y_val_predicted = lr.predict(noOneHotXVal)\n",
    "r2_score_lr = lr.score(noOneHotXVal, y_val)\n",
    "\n",
    "print('Linear regression without one hot encoding', r2_score_lr)\n",
    "\n",
    "# 0.3246299878844209"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "res = []\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_features = scaler.transform(X_train.values)\n",
    "X_train_minMax = pd.DataFrame(scaled_features, index=X_train.index, columns=X_train.columns)\n",
    "scaled_features = scaler.transform(X_test.values)\n",
    "X_test_minMax = pd.DataFrame(scaled_features, index=X_test.index, columns=X_test.columns)\n",
    "scaled_features = scaler.transform(X_val.values)\n",
    "X_val_minMax = pd.DataFrame(scaled_features, index=X_val.index, columns=X_val.columns)\n",
    "\n",
    "X_train_quick = X_train_minMax.head(5000)\n",
    "y_train_quick = y_train.head(5000)\n",
    "\n",
    "knnR = KNeighborsRegressor()\n",
    "\n",
    "clf = GridSearchCV(estimator=knnR,\n",
    "                   param_grid={'weights':('uniform', 'distance'),\n",
    "                               'n_neighbors':[1, 2],\n",
    "                               'metric':('euclidean', 'manhattan', 'chebyshev', 'minkowski'\n",
    "                                            , 'wminkowski', 'seuclidean', 'mahalanobis')},\n",
    "                   n_jobs=-1,\n",
    "                   return_train_score=True)\n",
    "modelCV = clf.fit(X_train_quick, y_train_quick)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultat  0.6661724567683306\n"
     ]
    },
    {
     "data": {
      "text/plain": "({'metric': 'manhattan', 'n_neighbors': 2, 'weights': 'uniform'},\n -0.2350427154385631,\n {'mean_fit_time': array([0.12135758, 0.1203784 , 0.14790883, 0.11626387, 0.12372189,\n         0.10296874, 0.11151485, 0.14813619, 0.13318505, 0.12921667,\n         0.1418191 , 0.14140544, 0.18127537, 0.12378449, 0.14481168,\n         0.16184535, 0.01981096, 0.03151054, 0.0195622 , 0.01336799,\n         0.01925907, 0.01978159, 0.01870418, 0.02419062, 0.02251945,\n         0.0214438 , 0.01965203, 0.01576548]),\n  'std_fit_time': array([0.03596158, 0.02979526, 0.03888881, 0.01317424, 0.02316571,\n         0.00566565, 0.00942106, 0.03426813, 0.04788441, 0.02157948,\n         0.03727414, 0.02242042, 0.04803039, 0.02621124, 0.03677444,\n         0.06041266, 0.00507728, 0.00923879, 0.00908921, 0.00124371,\n         0.0050534 , 0.00348354, 0.01227654, 0.00679751, 0.00243126,\n         0.00512855, 0.00670037, 0.00440319]),\n  'mean_score_time': array([0.92081866, 0.88533816, 1.05022178, 1.06049409, 0.98348026,\n         0.85680585, 1.07111859, 1.11108804, 1.07870784, 1.02865043,\n         1.42065802, 1.42123733, 0.99360552, 0.92964077, 1.1546226 ,\n         1.23100076, 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.        ]),\n  'std_score_time': array([0.11922692, 0.11483087, 0.11429653, 0.02986597, 0.03337579,\n         0.1065083 , 0.0932615 , 0.0857663 , 0.14316896, 0.06501985,\n         0.12420077, 0.11012764, 0.1409558 , 0.12540748, 0.11235787,\n         0.19608053, 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.        ]),\n  'param_metric': masked_array(data=['euclidean', 'euclidean', 'euclidean', 'euclidean',\n                     'manhattan', 'manhattan', 'manhattan', 'manhattan',\n                     'chebyshev', 'chebyshev', 'chebyshev', 'chebyshev',\n                     'minkowski', 'minkowski', 'minkowski', 'minkowski',\n                     'wminkowski', 'wminkowski', 'wminkowski', 'wminkowski',\n                     'seuclidean', 'seuclidean', 'seuclidean', 'seuclidean',\n                     'mahalanobis', 'mahalanobis', 'mahalanobis',\n                     'mahalanobis'],\n               mask=[False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False],\n         fill_value='?',\n              dtype=object),\n  'param_n_neighbors': masked_array(data=[1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1,\n                     2, 2, 1, 1, 2, 2, 1, 1, 2, 2],\n               mask=[False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False],\n         fill_value='?',\n              dtype=object),\n  'param_weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance',\n                     'uniform', 'distance', 'uniform', 'distance'],\n               mask=[False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False],\n         fill_value='?',\n              dtype=object),\n  'params': [{'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'euclidean', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'euclidean', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'manhattan', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'manhattan', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'chebyshev', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'chebyshev', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'chebyshev', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'chebyshev', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'minkowski', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'minkowski', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'minkowski', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'minkowski', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'wminkowski', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'wminkowski', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'wminkowski', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'wminkowski', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'seuclidean', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'seuclidean', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'seuclidean', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'seuclidean', 'n_neighbors': 2, 'weights': 'distance'},\n   {'metric': 'mahalanobis', 'n_neighbors': 1, 'weights': 'uniform'},\n   {'metric': 'mahalanobis', 'n_neighbors': 1, 'weights': 'distance'},\n   {'metric': 'mahalanobis', 'n_neighbors': 2, 'weights': 'uniform'},\n   {'metric': 'mahalanobis', 'n_neighbors': 2, 'weights': 'distance'}],\n  'split0_test_score': array([-2.96645665, -2.96645665, -1.20013967, -2.83059126, -2.86328497,\n         -2.86328497, -0.80319183, -2.48903113, -4.52237639, -4.52237639,\n         -1.34967585, -3.21076879, -2.96645665, -2.96645665, -1.20013967,\n         -2.83059126,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'split1_test_score': array([-0.12462461, -0.12462461, -0.20197246, -0.203611  , -0.10397639,\n         -0.10397639, -0.19009718, -0.1955952 , -0.1245135 , -0.1245135 ,\n         -0.21122003, -0.23054322, -0.12462461, -0.12462461, -0.20197246,\n         -0.203611  ,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'split2_test_score': array([-0.18179026, -0.18179026, -0.0520373 , -0.05199105, -0.10442454,\n         -0.10442454, -0.05955324, -0.05731097, -0.48996254, -0.48996254,\n         -0.15011592, -0.14727216, -0.18179026, -0.18179026, -0.0520373 ,\n         -0.05199105,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'split3_test_score': array([-0.00450618, -0.00450618, -0.00562898, -0.0045687 , -0.00408063,\n         -0.00408063, -0.0047297 , -0.00378663, -0.01173373, -0.01173373,\n         -0.01076548, -0.0083042 , -0.00450618, -0.00450618, -0.00562898,\n         -0.0045687 ,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'split4_test_score': array([-0.16126914, -0.16126914, -0.11289878, -0.10672153, -0.04480262,\n         -0.04480262, -0.11764162, -0.09836344, -0.11724824, -0.11724824,\n         -0.10175854, -0.11069395, -0.16126914, -0.16126914, -0.11289878,\n         -0.10672153,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'mean_test_score': array([-0.68772937, -0.68772937, -0.31453544, -0.63949671, -0.62411383,\n         -0.62411383, -0.23504272, -0.56881747, -1.05316688, -1.05316688,\n         -0.36470717, -0.74151647, -0.68772937, -0.68772937, -0.31453544,\n         -0.63949671,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan,         nan,         nan,\n                 nan,         nan,         nan]),\n  'std_test_score': array([1.14101829, 1.14101829, 0.44764966, 1.09754613, 1.12022765,\n         1.12022765, 0.29066722, 0.96215811, 1.742156  , 1.742156  ,\n         0.49682748, 1.23668624, 1.14101829, 1.14101829, 0.44764966,\n         1.09754613,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'rank_test_score': array([10, 10,  2,  8,  6,  6,  1,  5, 15, 15,  4, 14, 10, 10,  2,  8, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 17, 28], dtype=int32),\n  'split0_train_score': array([1.        , 1.        , 0.74507576, 1.        , 1.        ,\n         1.        , 0.74546873, 1.        , 1.        , 1.        ,\n         0.74112569, 1.        , 1.        , 1.        , 0.74507576,\n         1.        ,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'split1_train_score': array([0.99999987, 0.99999987, 0.74610622, 0.99999994, 0.99999987,\n         0.99999987, 0.74690545, 0.99999994, 0.99999987, 0.99999987,\n         0.7397325 , 0.99999994, 0.99999987, 0.99999987, 0.74610622,\n         0.99999994,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'split2_train_score': array([0.99999987, 0.99999987, 0.74119509, 0.99999994, 0.99999987,\n         0.99999987, 0.74758681, 0.99999994, 0.99999987, 0.99999987,\n         0.73974454, 0.99999994, 0.99999987, 0.99999987, 0.74119509,\n         0.99999994,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'split3_train_score': array([0.99999852, 0.99999852, 0.7007233 , 0.99999926, 0.99999852,\n         0.99999852, 0.71984342, 0.99999926, 0.99999852, 0.99999852,\n         0.58836837, 0.99999926, 0.99999852, 0.99999852, 0.7007233 ,\n         0.99999926,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'split4_train_score': array([1.        , 1.        , 0.74617581, 1.        , 1.        ,\n         1.        , 0.74679329, 1.        , 1.        , 1.        ,\n         0.74520608, 1.        , 1.        , 1.        , 0.74617581,\n         1.        ,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'mean_train_score': array([0.99999965, 0.99999965, 0.73585523, 0.99999983, 0.99999965,\n         0.99999965, 0.74131954, 0.99999983, 0.99999965, 0.99999965,\n         0.71083544, 0.99999983, 0.99999965, 0.99999965, 0.73585523,\n         0.99999983,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan,        nan,        nan,\n                nan,        nan,        nan]),\n  'std_train_score': array([5.67368926e-07, 5.67368926e-07, 1.76600227e-02, 2.83684463e-07,\n         5.67368926e-07, 5.67368926e-07, 1.07599480e-02, 2.83684463e-07,\n         5.67368926e-07, 5.67368926e-07, 6.12663028e-02, 2.83684463e-07,\n         5.67368926e-07, 5.67368926e-07, 1.76600227e-02, 2.83684463e-07,\n                    nan,            nan,            nan,            nan,\n                    nan,            nan,            nan,            nan,\n                    nan,            nan,            nan,            nan])})"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = modelCV.predict(X_test_minMax.head(500))\n",
    "\n",
    "print('Resultat ', modelCV.score(X_train_minMax.head(500), y_train.head(500)))\n",
    "\n",
    "# plt.plot(range(10),range(10), 'r')\n",
    "# plt.title(\"Resposta predita vs observada\")\n",
    "# plt.xlabel(\"Resposta predita\")\n",
    "# plt.ylabel(\"Resposta observada\")\n",
    "# plt.scatter(y_test, y_pred)\n",
    "modelCV.best_params_ , modelCV.best_score_, modelCV.cv_results_\n",
    "#0.2188"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "#\n",
    "# regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "# print(regr.predict(X_test[:2]))\n",
    "# print(regr.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4288970143.827506\n",
      "-2409535329.85503\n"
     ]
    }
   ],
   "source": [
    "print(regr.score(X_val, y_val))\n",
    "print(regr.score(X_train, y_train))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}